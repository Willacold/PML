{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: []  \n",
    "Student ID: []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before starting, you will need to install the following libraries (on a conda environment with python==3.10):\n",
    "\n",
    "pip install jupyter\n",
    "pip install deepchem[torch]\n",
    "pip install matplotlib\n",
    "pip install torchvision\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchdiffeq import odeint\n",
    "import deepchem as dc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neural ODE? Part 1: The ODE\n",
    "\n",
    "This notebook will help you to understand the above question. First, let's start with understanding what an ODE (ordinary differential equation) is. An ODE is an equation that defines a relationship between a function and its derivative. A simple example is the following:\n",
    "\n",
    "$$\n",
    "\\frac{dg(x)}{dx} = f(x)\n",
    "$$\n",
    " \n",
    "We want to know the function $g(x)$, given some initial condition $g(0)$. Let's do a simple example. Suppose $f(x) = 2x$ and $g(0) = 0$, then we can solve this ODE fairly simply by integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Solve the above equation analytically and write the equation for g(x) below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! But the above equation was very simple. Not all equations are so easy to solve. In fact, many cannot be solved analytically. However, there are still ways to approximate them numerically, and that is where an ODE solver comes into play. In this notebook, we will use the odeint function from the torchdiffeq library to solve a simple ODE. There are other ODE solvers available, but we will eventually move to neural ODEs, and since we will be using the pytorch framework, it makes sense to stick with the torchdiffeq library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, let's take that same equation as before, and attempt to solve it numerically. We are not going to worry too much about the details of the odeint itself (if you'd like to learn more, see  [here](https://en.wikipedia.org/wiki/Numerical_methods_for_ordinary_differential_equations) to learn more), but instead focus on how we can use it to solve our equation. To use it, we will need to define our function $f(x)$, define our initial value/parameter vector $g(0)$, and a range of points we want to solve for. Then, we need to call the odeint function on the three parameters in the order (initial value, function, points).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Implement the function f(x) = 2x, and define a variable that calls the odeint function on this, the initial value g0, and the range of points x.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, g):\n",
    "    ### Start your code\n",
    "\n",
    "    ### End your code\n",
    "    return equation\n",
    "\n",
    "\n",
    "# Define the initial value\n",
    "g0 = torch.Tensor([0])\n",
    "\n",
    "# Define the range of points to solve for\n",
    "x = torch.linspace(-10, 10, 1000)\n",
    "\n",
    "sol = #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's take a look at our g(x) on this interval! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, sol)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, doesn't that look nice! We form a nice parabola, which is exactly what we should have. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move into incorporating ODEs into neural network, let's first take a look at what inspired this. Resnet ([Residual neural networks](https://en.wikipedia.org/wiki/Residual_neural_network)) add in the concept of a \"skip connection\". What this means, is that the network will pass inputs forwards past another layer, and combine it afterwards. This helped with the issue of training stability, and allowed for networks of hundreds of layers to be trained. Let's load up some data, and then we'll take a look at the specifics of Resnets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this is going to be a very intensive operation, so using a GPU is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the device to train the NN on. This tries Nvidia CUDA (GPU) first, then CPU. If you have an AMD GPU, there may be a way, but I am not certain, sorry.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device) #See which device we are using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load up the dataset we will train our Resnet on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Load the Data\n",
    "\n",
    "shuffle=True\n",
    "\n",
    "bsize = 512 #Set to a smaller batch size if you are running out of memory\n",
    "\n",
    "indices = list(range(50000))\n",
    "split = int(np.floor(0.1 * 50000))\n",
    "\n",
    "if shuffle:\n",
    "    np.random.seed(2025)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "\n",
    "use_cuda = True\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=bsize, sampler=train_sampler, **kwargs)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=bsize, sampler=valid_sampler, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=bsize, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset loaded, it's time to start building a Resnet. A Resnet is made up of Residual blocks, which have a skip connection that takes the data input to the block, runs it through some layers, then at the end, adds the input (with no modifications) to the end. Visually, it looks like the following picture (image from [Wikipedia](https://en.wikipedia.org/wiki/Residual_neural_network)):\n",
    "\n",
    "<img src=\"images/ResBlock.png\" style=\"width:300px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Write the forward pass for the basic Resnet Block\n",
    "\n",
    "The task is to write the forward pass for the above Resnet block. In this case, we will have a 2-layer Resnet block that accepts an input $x$ ($x$ in this case being an image from our dataset). The output $F(x)$ of the layer will be of the following form:\n",
    "\n",
    "$$\n",
    "\n",
    "h(x) = \\sigma(F(x) + x) = \\sigma_2(w_2(\\sigma_1(w_1x+b))+b+x)\n",
    "$$\n",
    "\n",
    "A few things to keep in mind here. First, both activation functions $\\sigma$ will be ReLU units. Second, keep in mind that the second ReLU activation will come after you add back in the input using the skip connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, units, skip_connection=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.l1 = nn.Sequential(nn.LazyLinear(units), nn.ReLU())\n",
    "        self.l2 = nn.Sequential(nn.LazyLinear(units))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Begin your code here\n",
    "\n",
    "        ### End your code here\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we put this into a Resnet network. This Resnet network will utilize four sets of layers, with each layer having 2 sets of Resnet blocks inside. In total, there are 8 Resnet blocks in this network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_model(nn.Module):\n",
    "    def __init__(self, resnet_block, layers, num_outputs = 10):\n",
    "        super(resnet_model, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.layer0 = nn.LazyLinear(64)\n",
    "        self.layer1 = self._make_layer(resnet_block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(resnet_block, 64, layers[1])\n",
    "        self.layer3 = self._make_layer(resnet_block, 64, layers[2])\n",
    "        self.layer4 = self._make_layer(resnet_block, 64, layers[3])\n",
    "        self.classifier_layer = nn.LazyLinear(num_outputs)\n",
    "\n",
    "    def _make_layer(self, resnet_block, units, resblock_layers):\n",
    "        layers = []\n",
    "        layers.append(resnet_block(units))\n",
    "        for i in range(1, resblock_layers):\n",
    "            layers.append(resnet_block(units))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.classifier_layer(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "model = resnet_model(BasicBlock, [2, 2, 2, 2], num_outputs=num_classes).to(device)\n",
    "\n",
    "#Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())  \n",
    "\n",
    "#Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we train the network. Let's see how it does! This could take a while, so feel free to go get a cup of coffee, do some other homework, or elsewise, while this is running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # print('Epoch: {}'.format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        print('Batch: {}'.format(i), end=\"\\r\") \n",
    "    \n",
    "        #Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "#Validation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! But we took 8 Resnet blocks to do this. Trying this on larger datasets, and you could quickly find yourself running out of time and memory. To solve this, let's take a look at a nerual ODE, and bridge the gap. First, recall that when making the Resent, we could write the output of a Resnet block in the form of:\n",
    "\n",
    "$$\n",
    "F(x) + x\n",
    "$$\n",
    "\n",
    "We have a hidden state in the network that is being passed along. Moreover, if you are familiar with numerical ODE solvers, this looks very much like using the Euler method to update. As such, we can think of neural ODEs as Resnets with an infinite (continuous) number of layers. Now lets go back to the very beginning of this notebook, and recall our simple example of an ODE:\n",
    "\n",
    "$$\n",
    "\\frac{dg(x)}{dx} = f(x)\n",
    "$$\n",
    "\n",
    "In this case, $f(x)$ is a function. Neural networks happen to have a nice property that they can approximate any function to an arbitrary degree ([universal function approximation](https://en.wikipedia.org/wiki/Universal_approximation_theorem)). Ergo, we can replace $f(x)$ with a neural network approximation thereof. In this case, our simple example would become:\n",
    "\n",
    "$$\n",
    "\\frac{dg(x)}{dx} = f_\\theta(h(x), x)\n",
    "$$\n",
    "\n",
    "Where $f_\\theta(h(x), x)$ is a neural network approximation of our original function $f(x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this idea, let's build a neural ODE network. First, we will need to define the function. The following does so, defining our parameterized neural network $f_\\theta(h(x),x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODENN_Function(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(ODENN_Function, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.LazyLinear(units), \n",
    "            nn.ReLU(), \n",
    "            nn.LazyLinear(units), \n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that, we need to create the actual ODE layer, which is similar to a Resnet block.\n",
    "\n",
    "### Q4: Create the forward pass for the ODE layer\n",
    "\n",
    "The forward pass is just the output of the ODE numerical solver. The forward pass will take in input $x$. This input, along with and the \"range of points to solve for\" (integration_time) and the function (f_x) will be sent into an ODE solver. The output of this solver will be the output of the ODE layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEBlock(nn.Module):\n",
    "    def __init__(self, f_x):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.f_x = f_x\n",
    "        self.integration_time = torch.Tensor([0,1]).float()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Begin your code here\n",
    "\n",
    "        ### End your code here\n",
    "        return out[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Make the forward pass for the neural ODE\n",
    "\n",
    "The neural ODE is structured much like the Resnet, but because of the continuous layer structure, you don't need to manually define each layer as you do in Resnet. Using the Resnet as a template, create the forward pass for the neural ODE. Rememebr that we don't have a discrete number of Resnet blocks, but rather a continuous ODE block. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODENet(nn.Module):\n",
    "    def __init__(self, units, num_outputs):\n",
    "        super(ODENet, self).__init__()\n",
    "        fx = ODENN_Function(units)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.l1 = nn.LazyLinear(units)\n",
    "        self.ode_block = ODEBlock(fx)\n",
    "        self.classifier_layer = nn.LazyLinear(num_outputs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ### Begin your code here\n",
    "\n",
    "        ### End your code here\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "model = ODENet(units=64, num_outputs=num_classes).to(device)\n",
    "\n",
    "#Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())  \n",
    "\n",
    "#Train the model\n",
    "total_step = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # print('Epoch: {}'.format(epoch))\n",
    "    for i, (images, labels) in enumerate(train_loader): \n",
    "        print('Batch: {}'.format(i), end=\"\\r\") \n",
    "    \n",
    "        #Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        #Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        #Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
    "                    .format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "#Validation\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in valid_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite similar to the Resnet! This example was fairly quick, but depending on your dataset, this could take much longer, or shorter, than a given Resnet model to train.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at one last thing. Because neural ODEs have a layer based upon an ODE solver, they are particularly good where there are dynamic equations (regular ODEs and the like) involved in the data. This is especially prevalent in physical processes, such as fluid dynamics, glacial accumulation, and other phenomena. Now we will take a quick look at an example on real world physical data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Section: Physical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a chemistry dataset, the Delaney solubility dataset, provided by the [DeepChem](https://github.com/deepchem/deepchem/tree/master) library. The task will be to predict the solubility of a molecule based upon its extended-connectivity fingerprint (ECFP). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ECFP is a topological fingerprint for molecular characterization, meaning it contains information about the structure of various molecules [[1]](https://pubs.acs.org/doi/10.1021/ci100050t). The idea behind this task is that we can make use of this information in order to, based upon the features that can be extracted from the ECFP (features such as molecule length, etc), we may find a dependence between these structural features and the solubility.\n",
    "\n",
    "But why use a neural ODE? Well, while we can't say for certain it is the case here, many phenomena in chemistry are governed by ODEs and PDEs. Some examples of such behaviour in solubility and related is the dependence of solubility on pressure, which is defined by: $\\frac{\\partial \\ln N_i}{\\partial P}_T = -\\frac{V_{i, q} - V_{i, cr}}{RT}$, or the rate of dissolution, which is defined by: $\\frac{dm}{dt} = A \\frac{D}{d}(C_s - C_n)$ (see [here](en.wikipedia.org/wiki/Solubility) for more information on solubility). Owing to this, a good hunch is that the relationship between the structure and the solubility may also have an ODE relation, hence, a neural ODE may be a good method to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got a little of the theory out of the way, let's move into the practical. First, let's load the dataset, and then define the loss function to use. DeepChem recommends using an $L_2$ Loss, and using the Pearson R2 score as a metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='ECFP', splitter='random')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataset, we need to train our neural ODE to predict the solubility given an ECFP. DeepChem comes with a very nice wrapper for training models, and so all we need to do is create a model and load it into the wrapper, and we can go and train.\n",
    "\n",
    "### Q6 (BONUS): Create a Neural ODE model with 512 neurons per layer, and a single output neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ODE_model = ### Your code here\n",
    "\n",
    "deepchem_model = dc.models.TorchModel(ODE_model, dc.models.losses.L2Loss())\n",
    "\n",
    "deepchem_model.fit(train_dataset, nb_epoch=50)\n",
    "print('training set score:', deepchem_model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', deepchem_model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! It did decently well. While we can always do better, I hope the simplicity with which this could be done will inspire you to use neural ODEs in more problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Checklist\n",
    "\n",
    "There are a total of 5 mandatory questions in HW assignment, with 1 bonus question.\n",
    "\n",
    "This list is provided to ensure you don't forget any of them:\n",
    "\n",
    "Q1          []  \n",
    "\n",
    "Q2          []  \n",
    "\n",
    "Q3          []  \n",
    "\n",
    "Q4          []  \n",
    "\n",
    "Q5          []  \n",
    "\n",
    "Q6 (BONUS)  []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
